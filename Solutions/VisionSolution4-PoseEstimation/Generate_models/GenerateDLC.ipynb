{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4caa655",
   "metadata": {},
   "source": [
    "## Steps for generating object_detect.dlc\n",
    "\n",
    "For this demo, a YoloNAS model is used. You can read more about this model in VisionSolution1-YoloNasSSD Readme.\n",
    "\n",
    "**Installing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd4b5b-7629-4471-930a-915f195b6c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install super-gradients==3.1.2\n",
    "!pip3 install cython\n",
    "!pip3 install yacs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663435cc-03d9-4fc9-b4a7-bd6c13db80cc",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3613d7-d1a1-453c-acdb-6d80451f2a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip -q --show-progress\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip -q --show-progress\n",
    "!unzip val2017.zip\n",
    "!unzip coco2017labels.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769ba01-fcb8-4022-a917-1c49c023c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir('val2017')\n",
    "for file in files[50:]:\n",
    "    os.remove(\"val2017/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d3295-3b46-4da6-9ae3-ccff6ea25eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf coco\n",
    "rm -rf coco2017labels.zip\n",
    "rm -rf val2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5d982-47eb-406f-a454-87f71193ba95",
   "metadata": {},
   "source": [
    "#### Downloading the YOLO_Nas Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading Model from git repo\n",
    "import torch\n",
    "# Load model with pretrained weights\n",
    "from super_gradients.training import models\n",
    "from super_gradients.common.object_names import Models\n",
    "\n",
    "model = models.get(Models.YOLO_NAS_S, pretrained_weights=\"coco\")\n",
    "\n",
    "# Prepare model for conversion\n",
    "# Input size is in format of [Batch x Channels x Width x Height] where 640 is the standard COCO dataset dimensions\n",
    "model.eval()\n",
    "model.prep_model_for_conversion(input_size=[1, 3, 320, 320])\n",
    "\n",
    "# Create dummy_input\n",
    "dummy_input = torch.randn([1, 3, 320, 320], device=\"cpu\")\n",
    "\n",
    "# Convert model to onnx\n",
    "torch.onnx.export(model, dummy_input, \"yolo_nas_s.onnx\", opset_version=11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1d8180c",
   "metadata": {},
   "source": [
    "#### Converting to DLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21abc81-9aeb-426a-bda3-4c316c8c1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SNPE_ROOT']=\"/local/mnt/workspace/snpe/2.29.0.241129/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e7ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "snpe-onnx-to-dlc -i yolo_nas_s.onnx -o ../app/src/main/assets/yolo_nas_s.dlc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2675610",
   "metadata": {},
   "source": [
    "## Quantizing Yolo_nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEPS to preprocess images\n",
    "\n",
    "def preprocess(original_image):\n",
    "    resized_image = cv2.resize(original_image, (320, 320))\n",
    "    resized_image = resized_image/255\n",
    "    return resized_image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "dataset_path = \"val2017/\"\n",
    "\n",
    "os.makedirs('rawYoloNAS', exist_ok=True)\n",
    "\n",
    "filenames=[]\n",
    "for path in os.listdir(dataset_path)[:5]:\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dataset_path, path)):\n",
    "        filenames.append(os.path.join(dataset_path, path))\n",
    "\n",
    "for filename in filenames:\n",
    "    original_image = cv2.imread(filename)\n",
    "    img = preprocess(original_image)\n",
    "    img = img.astype(np.float32)\n",
    "    img.tofile(\"rawYoloNAS/\"+filename.split(\"/\")[-1].split(\".\")[0]+\".raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find rawYoloNAS -name *.raw > YoloInputlist.txt\n",
    "cat YoloInputlist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b97591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "snpe-dlc-quantize --input_dlc ../app/src/main/assets/yolo_nas_s.dlc --input_list YoloInputlist.txt --output_dlc ../app/src/main/assets/Quant_yoloNas_s_320_online.dlc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a6a6e",
   "metadata": {},
   "source": [
    "snpe-dlc-graph-prepare requires device htp_soc info. \n",
    "\n",
    "So, depending on the device --htp_socs needs to be changed (sm8550 or sm8650 or sm8750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f715d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "snpe-dlc-graph-prepare --input_dlc ../app/src/main/assets/Quant_yoloNas_s_320_online.dlc --set_output_tensors 885,893 --output_dlc ../app/src/main/assets/Quant_yoloNas_s_320.dlc --htp_socs sm8750"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e8f6ab0",
   "metadata": {},
   "source": [
    "\n",
    "## How to change the object-detect model ? \n",
    "\n",
    "Object detection models are highly dependant on model architecture, and the pre-processing requirements vary a lot from model to model. \n",
    "If user intends to use a different model e.g. YoloV5, following steps should be followed : \n",
    "\n",
    "- Ensure QualcommÂ® Neural Processing SDK supports the operations in selected model\n",
    "- Study the pre processing, and post processing requirements for the selected model\n",
    "- Most object detection models operate in RGB space. Input camera YUV buffers need to be converted to RGB basd on model requirements \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d32ad59",
   "metadata": {},
   "source": [
    "# Info about HRNET\n",
    "\n",
    "HRNET model is State-of-the-art model for human pose estimation. It has good accuracy for results with single person, but has lower accuracy for multiple persons. To enhance that, HRNET uses object-detect model to identify a single person in a frame and then give the data to HRNET to get pose of that person. In this solution, we use MobileNetSSD for detecting human and then give the preprocesssed data to HRNET to achieve better accuracy for pose estimation.\n",
    "\n",
    "HRNET dlc takes 256x192x3 flattened array as input and returns output of dims 17x64x48. HRNET generates heatmap for 17 human joints and each heatmap is of size 64x48."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9026cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf HRNet-Human-Pose-Estimation/\n",
    "git clone https://github.com/HRNet/HRNet-Human-Pose-Estimation.git\n",
    "git checkout 00d7bf72f56382165e504b10ff0dddb82dca6fd2\n",
    "cp hrnet.patch HRNet-Human-Pose-Estimation/\n",
    "cd HRNet-Human-Pose-Estimation/\n",
    "patch -p1 < ./hrnet.patch\n",
    "cd lib\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013adf2-8bd5-46e2-8d3d-7a50dc79d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p mode_binaries\n",
    "cd mode_binaries\n",
    "wget https://github.com/quic/aimet-model-zoo/releases/download/hrnet-posenet/hrnet_posenet_FP32.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a0193-0b98-477f-843f-136f12345af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.11.0\n",
    "!pip install torchvision==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Getting onnx from pth model for hrnet requires a different setup  #\n",
    "# python 3.6                                                        #\n",
    "# torch 1.10.1                                                      #\n",
    "# torchvision 0.11.2                                                #\n",
    "#####################################################################\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "# from config import cfg\n",
    "import os\n",
    "import os.path as osp\n",
    "import urllib.request\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "lib_path = osp.join(os.getcwd(), 'HRNet-Human-Pose-Estimation/lib')\n",
    "sys.path.insert(0, lib_path)\n",
    "if not os.path.exists(\"model_binaries\"):\n",
    "    os.makedirs(\"model_binaries\")\n",
    "##Getting .pth file\n",
    "OPTIMIZED_CHECKPOINT_URL = (\n",
    "    # \"https://github.com/quic/aimet-model-zoo/releases/download/hrnet-posenet/hrnet_posenet_FP32.pth\"\n",
    "    \"https://github.com/quic/aimet-model-zoo/releases/download/hrnet-posenet/\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(f\"./model_binaries/hrnet_posenet_FP32.pth\"):\n",
    "    urllib.request.urlretrieve(\n",
    "        f\"{OPTIMIZED_CHECKPOINT_URL}/hrnet_posenet_FP32.pth\",\n",
    "        f\"model_binaries/hrnet_posenet_FP32.pth\",\n",
    "    )\n",
    "\n",
    "\n",
    "input_shape = (1, 3, 256, 192)\n",
    "dummy_input = torch.randn(input_shape)\n",
    "model = torch.load(\"model_binaries/hrnet_posenet_FP32.pth\")\n",
    "model.to('cpu')\n",
    "\n",
    "onnx_model_name = \"model_binaries/AIMET_HRNET_posnet.onnx\"\n",
    "\n",
    "opset = 11\n",
    "\n",
    "torch.onnx.export(\n",
    "    model.cpu(),\n",
    "    dummy_input,\n",
    "    onnx_model_name,\n",
    "    verbose=True,\n",
    "    do_constant_folding=True,\n",
    "    export_params=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    opset_version=opset\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f051a58",
   "metadata": {},
   "source": [
    "## Steps for generating HRNET dlc for int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "snpe-onnx-to-dlc -i model_binaries/AIMET_HRNET_posnet.onnx -o ../app/src/main/assets/hrnet.dlc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f63bc1b",
   "metadata": {},
   "source": [
    "## Steps for Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "preproc = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2,os\n",
    "\n",
    "dataset_path = \"val2017/\"\n",
    "\n",
    "os.makedirs('rawHRNET', exist_ok=True)\n",
    "\n",
    "filenames=[]\n",
    "for path in os.listdir(dataset_path)[:5]:\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dataset_path, path)):\n",
    "        filenames.append(os.path.join(dataset_path, path))\n",
    "print(filenames)\n",
    "\n",
    "for filename in filenames:\n",
    "    orig_img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img,(256,192),\n",
    "                   interpolation = cv2.INTER_LINEAR)\n",
    "    model_input = preproc(img).unsqueeze(0)\n",
    "\n",
    "    model_input = model_input.cpu().detach().numpy()\n",
    "    model_input = model_input.transpose(0,2,3,1)     \n",
    "    fid = open(\"rawHRNET/\"+filename.split(\"/\")[-1].split(\".\")[0]+\".raw\", 'wb')\n",
    "    model_input.tofile(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "\n",
    "find rawHRNET -name *.raw > HRNET_input_list.txt\n",
    "snpe-dlc-quantize --input_dlc ../app/src/main/assets/hrnet.dlc --input_list HRNET_input_list.txt --axis_quant --output_dlc ../app/src/main/assets/hrnet_axis_int8.dlc --enable_htp --htp_socs sm8750\n",
    "snpe-dlc-info --input_dlc ../app/src/main/assets/hrnet_axis_int8.dlc > hrnet_axis_int8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20eefc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
