{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85201f90-f251-4c26-a21a-977a68bfc723",
   "metadata": {},
   "source": [
    "### Loading the Necessary Github Repos\n",
    "* <b> Please double check if git repos are cloned properly, otherwise notebook will run into multiple issues.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9a30b-75aa-4800-8139-6c3ac6624c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx\n",
    "!pip install onnx_tf\n",
    "!git clone https://github.com/usefulsensors/openai-whisper.git\n",
    "!git clone https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d0645-baea-46f3-9122-35894ecdda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp whisper.patch whisper/\n",
    "cd whisper/\n",
    "patch -p1 < ./whisper.patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2665aca-9241-4283-a96f-e3beb31572ce",
   "metadata": {},
   "source": [
    "### Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6bece-dce6-416b-bc7a-8cda0a72e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers==4.29.2 \n",
    "!pip3 install safetensors==0.3.0\n",
    "!pip3 install pyyaml==5.3\n",
    "!pip3 install numpy==1.22.2\n",
    "!pip3 install torchvision==0.15.2\n",
    "!pip3 install packaging==21.3\n",
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3750cd1-4bbb-493d-93da-5c27318c9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import tqdm\n",
    "#from onnx_tf.backend import prepare\n",
    "from whisper.whisper import load_model\n",
    "from whisper.whisper.audio import load_audio, log_mel_spectrogram,pad_or_trim,N_FRAMES, SAMPLE_RATE\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "#load openai->whisper(pytorch)->tiny model\n",
    "tiny_model = load_model(\"tiny\")\n",
    "\n",
    "#Export to onnx format\n",
    "torch.onnx.export(tiny_model.encoder,torch.randn(1,80,3000).to(device), \"./whisper_encoder.onnx\",opset_version=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d17617-b1fa-4693-8c3d-d78250206eab",
   "metadata": {},
   "source": [
    "### Getting the Encoder ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b59f9-0cae-42ac-bc48-2d58acfc0fd5",
   "metadata": {},
   "source": [
    "### IDEA\n",
    "**Basic Idea will be**\n",
    "- Encoder will run as dlc\n",
    "- Decoder will run as tflite Model\n",
    "- The output generated from decoder will be added with output from previous decoder instance and then multiplied with the encoder result\n",
    "\n",
    "![modelArchitecture](image-assets/whisper_model_Architecture.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085ee3a-ab4c-4930-9a4e-e5a43ac18358",
   "metadata": {},
   "source": [
    "#### Normal Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7114916-9d31-417a-95ae-cdd4abff9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.8.1 \n",
    "!pip3 install onnxruntime\n",
    "!pip3 install tensorflow==2.10.1\n",
    "!pip3 install tflite==2.3.0\n",
    "!pip3 install soundfile\n",
    "!pip3 install librosa\n",
    "!pip3 install numpy==1.22\n",
    "#!pip3 install numpy==1.18.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\") \n",
    "# generate token ids\n",
    "predicted_ids = model.generate(**input_features,decoder_input_ids=torch.tensor([[50258, 50259, 50359, 50363]]))\n",
    "# decode token ids to text\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a9e4b-a1ae-4a34-8417-dba3c60d292a",
   "metadata": {},
   "source": [
    "#### Getting The Special Token Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc79c6-98c6-462f-86fe-295fff052fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens=['<|endoftext|>', '<|startoftranscript|>', '<|en|>', '<|zh|>', '<|de|>', '<|es|>', '<|ru|>', '<|ko|>', '<|fr|>', '<|ja|>', '<|pt|>', '<|tr|>', '<|pl|>', '<|ca|>', '<|nl|>', '<|ar|>', '<|sv|>', '<|it|>', '<|id|>', '<|hi|>', '<|fi|>', '<|vi|>', '<|he|>', '<|uk|>', '<|el|>', '<|ms|>', '<|cs|>', '<|ro|>', '<|da|>', '<|hu|>', '<|ta|>', '<|no|>', '<|th|>', '<|ur|>', '<|hr|>', '<|bg|>', '<|lt|>', '<|la|>', '<|mi|>', '<|ml|>', '<|cy|>', '<|sk|>', '<|te|>', '<|fa|>', '<|lv|>', '<|bn|>', '<|sr|>', '<|az|>', '<|sl|>', '<|kn|>', '<|et|>', '<|mk|>', '<|br|>', '<|eu|>', '<|is|>', '<|hy|>', '<|ne|>', '<|mn|>', '<|bs|>', '<|kk|>', '<|sq|>', '<|sw|>', '<|gl|>', '<|mr|>', '<|pa|>', '<|si|>', '<|km|>', '<|sn|>', '<|yo|>', '<|so|>', '<|af|>', '<|oc|>', '<|ka|>', '<|be|>', '<|tg|>', '<|sd|>', '<|gu|>', '<|am|>', '<|yi|>', '<|lo|>', '<|uz|>', '<|fo|>', '<|ht|>', '<|ps|>', '<|tk|>', '<|nn|>', '<|mt|>', '<|sa|>', '<|lb|>', '<|my|>', '<|bo|>', '<|tl|>', '<|mg|>', '<|as|>', '<|tt|>', '<|haw|>', '<|ln|>', '<|ha|>', '<|ba|>', '<|jw|>', '<|su|>', '<|translate|>', '<|transcribe|>', '<|startoflm|>', '<|startofprev|>', '<|nocaptions|>', '<|notimestamps|>']\n",
    "\n",
    "dict={50257+i:special_tokens[i] for i in range(0,len(special_tokens))}\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86684ee2-5d44-4f87-8be8-dce92a8c3028",
   "metadata": {},
   "source": [
    "#### Getting Different Token ids for Different Tasks\n",
    "- Right Now I've used For english language Transcribtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf65713-2fa5-4667-aa6e-218662994ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoConfig, AutoProcessor\n",
    "\n",
    "\n",
    "model = \"openai/whisper-tiny\"\n",
    "config = AutoConfig.from_pretrained(model)\n",
    "processor = AutoProcessor.from_pretrained(model)\n",
    "\n",
    "# English transcription\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "print(forced_decoder_ids)\n",
    "# forced_decoder_ids is of the format [(1, 50259), (2, 50359), (3, 50363)] and needs to be\n",
    "# of the format [50258, 50259, 50359, 50363] where 50258 is the start token id\n",
    "forced_decoder_ids = [config.decoder_start_token_id] + list(map(lambda token: token[1], forced_decoder_ids))\n",
    "print(forced_decoder_ids)\n",
    "# If you don't want to provide specific decoder input ids or you want\n",
    "# Whisper to predict the output language and task, you can set\n",
    "# forced_decoder_ids = [config.decoder_start_token_id]\n",
    "# [50258]\n",
    "\n",
    "# decoder input ids\n",
    "decoder_input_ids = np.array([forced_decoder_ids], dtype=np.int32)\n",
    "print(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993f9b2-6a6b-4915-a9e9-1ca3eeef0142",
   "metadata": {},
   "source": [
    "## Getting the tflite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360148c8-9423-42a2-b5cb-c5660b71731f",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bc9e4-e1ec-4ced-8da5-8d522c1165f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "# loading the processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "!rm -rf input_features\n",
    "os.makedirs('input_features',exist_ok=True)\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "print(ds)\n",
    "for i in range(25):\n",
    "    sample = ds[i][\"audio\"]\n",
    "    input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"np\") \n",
    "    \n",
    "    inp_val=input_features.input_features.astype(np.float32)\n",
    "    #Need to transpose the input\n",
    "    print(inp_val.shape)\n",
    "    updated_inp_val=inp_val.transpose(0,2,1)\n",
    "    print(updated_inp_val.shape)\n",
    "    with open(\"input_features/inp_val_\"+str(i)+\".raw\", 'wb') as f:\n",
    "        updated_inp_val.tofile(f)\n",
    "\n",
    "\n",
    "#Creating list.txt\n",
    "with open(\"list.txt\",'w') as f:\n",
    "    for i in range(25):\n",
    "        f.write(\"x.1:=input_features/inp_val_\"+str(i)+\".raw\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54418e2f-49e7-4b15-ac19-0eb614ce388e",
   "metadata": {},
   "source": [
    "##### Running the Decoder Model\n",
    "- Take the last_hidden_state of the encoder model\n",
    "- Then take the initial decoder_input_ids then one by one add\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0622ad7-a456-4dd7-a805-3bcc90f12e17",
   "metadata": {},
   "source": [
    "##### Tflite Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a16c4-7890-402b-ba5f-34e5696c0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "        AutoTokenizer\n",
    "    )\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "tflite_model_path='openai-whisper/models/whisper-decoder-tiny.tflite'\n",
    "#tflite_model_path='/content/whisper-decoder_main-int8.tflite'\n",
    "print(tflite_model_path)\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "\n",
    "def decoder_block_tflite(encoder_hidden_states):\n",
    "    \n",
    "    decoder_input_ids = torch.tensor([50258, 50259, 50359, 50363])\n",
    "    decoder_input_ids = tf.expand_dims(decoder_input_ids, 0)\n",
    "    \n",
    "    input_tensor_1 = interpreter.get_input_details()[0]['index']\n",
    "    \n",
    "    interpreter.set_tensor(input_tensor_1, encoder_hidden_states)\n",
    "    \n",
    "    input_tensor_2 = interpreter.get_input_details()[1]['index']\n",
    "    interpreter.resize_tensor_input(input_tensor_2, decoder_input_ids.shape)\n",
    "    # Allocate memory for input and output tensors\n",
    "    interpreter.allocate_tensors()\n",
    "    interpreter.set_tensor(input_tensor_2, decoder_input_ids)\n",
    "    output_tensor = interpreter.get_output_details()[0]['index']\n",
    "    start_tokens = [50258, 50259, 50359, 50363] \n",
    "    tokens = start_tokens\n",
    "    while(True):\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_tensor) \n",
    "        # print(output_data.shape)\n",
    "        # print(output_data)\n",
    "        cleaned = np.argmax(output_data, axis=-1)\n",
    "        # print(\"cleaned\",cleaned)\n",
    "        last_token = cleaned[0,-1]\n",
    "        # print(\"Last Token\",last_token)\n",
    "        tokens.append(last_token)\n",
    "        # print(\"Updated tokens:\",tokens)\n",
    "        new_value = tf.constant([last_token], dtype=tf.int64)\n",
    "        new_value = tf.reshape(new_value, (1,1))\n",
    "        decoder_input_ids = tf.concat([decoder_input_ids, new_value], axis=1)\n",
    "        input_tensor_2 = interpreter.get_input_details()[1]['index']\n",
    "        interpreter.resize_tensor_input(input_tensor_2, decoder_input_ids.shape)\n",
    "        # Allocate memory for input and output tensors\n",
    "        interpreter.allocate_tensors()\n",
    "        interpreter.set_tensor(input_tensor_2, decoder_input_ids)\n",
    "        if last_token == 50257:\n",
    "          break\n",
    "    \n",
    "    \n",
    "    return tokenizer.batch_decode(np.expand_dims(tokens, axis=0), skip_special_tokens=True)[0]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d923d62d-28fe-43d1-b86b-158de52e7940",
   "metadata": {},
   "source": [
    "#### Getting the encoder DLC Model\n",
    "- The above ONNX model is running fine\n",
    "- Now Converting to DLC and checking how it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbc4e7-3fdf-4d6b-be09-fed18d45bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SNPE_ROOT']=\"/local/mnt/workspace/snpe/2.29.0.241129\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14935c-b4ae-41cf-90d1-e6efceebeb2e",
   "metadata": {},
   "source": [
    "#### Getting the Fp32 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d7bff-4e45-460b-bf42-1c63fe552c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install protobuf==3.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f4b82-166e-464f-953c-65c6cc75f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "\n",
    "snpe-onnx-to-dlc -i whisper_encoder.onnx -d x.1 1,80,3000 -o whisper_tiny_encoder_fp32.dlc\n",
    "snpe-dlc-info -i whisper_tiny_encoder_fp32.dlc > whisper_tiny_encoder_fp32.txt\n",
    "snpe-dlc-viewer -i whisper_tiny_encoder_fp32.dlc -s whisper_tiny_encoder_fp32.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4f7c0-ad27-4a02-b660-3f9d0d4cd11a",
   "metadata": {},
   "source": [
    "#### Creating w8a16 model\n",
    "* Choose --htp_socs based on the end device where model will be deployed. Example sm8750 or sm8650 or sm8550\n",
    "* --optimizations cle --axis_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad99d7-f80b-4f0b-b0c8-42b36f32a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "\n",
    "snpe-dlc-quantize --input_dlc whisper_tiny_encoder_fp32.dlc --input_list list.txt  --output_dlc whisper_tiny_encoder_w8a16.dlc --weights_bitwidth 8 --act_bitwidth 16 --enable_htp --htp_socs sm8750\n",
    "\n",
    "snpe-dlc-info -i whisper_tiny_encoder_w8a16.dlc > whisper_tiny_encoder_w8a16.txt\n",
    "snpe-dlc-viewer -i whisper_tiny_encoder_w8a16.dlc -s whisper_tiny_encoder_w8a16.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91092d-7445-4848-9265-e75823ff2960",
   "metadata": {},
   "source": [
    "### Inferencing the FP32 Model on linux x86 machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3a882-8c73-45ea-a4e6-e15d0f14e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf OUTPUT_Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b2a4d-5409-42cd-8ef5-21b09c90509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $SNPE_ROOT/bin/envsetup.sh\n",
    "\n",
    "snpe-net-run --container whisper_tiny_encoder_fp32.dlc --input_list list.txt --output_dir OUTPUT_Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713afd92-252a-4feb-be77-3c84b035342f",
   "metadata": {},
   "source": [
    "#### Checking the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81beaffe-8d39-4e9e-bb38-7e4e8ce5fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "folder = [\"OUTPUT_Encoder\"]\n",
    "\n",
    "\n",
    "for j in range(0,1):\n",
    "    for result_path in glob.glob(os.path.join(folder[j], '*')):\n",
    "        if \".log\" not in result_path:\n",
    "            last_hidden_state = np.fromfile(result_path+'/599.raw', dtype=\"float32\")\n",
    "            \n",
    "            encoder_hidden_states=last_hidden_state.reshape((1,1500,384))\n",
    "            print(decoder_block_tflite(encoder_hidden_states))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a9e14-e402-4d10-a7b6-ce07effd2d4a",
   "metadata": {},
   "source": [
    "## Inferencing on Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599e243-a46e-48a3-9529-9aadcfbfb933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "adb devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749fb7a8-2eb6-43ef-9c08-aa59e496517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SNPE_ROOT']=\"/local/mnt/workspace/snpe/2.29.0.241129\" #set up your snpe path here.\n",
    "os.environ['RAW_FILE_FOLDER']=\"input_features\"\n",
    "os.environ['FOLDER_WITH_ARTIFACTS']=\"whisper\"\n",
    "os.environ['DLCFP32']=\"whisper_tiny_encoder_fp32.dlc\"\n",
    "os.environ['DLCA8W16']=\"whisper_tiny_encoder_w8a16.dlc\"\n",
    "os.environ['DLCA8W8']=\"whisper_tiny_encoder_w8a8.dlc\"\n",
    "os.environ['DLCA16W16']=\"whisper_tiny_encoder_w16a16.dlc\"\n",
    "os.environ['TARGET_INPUT_LIST']=\"list.txt\"\n",
    "os.environ['ONDEVICE_FOLDER']=\"whisper\"\n",
    "os.environ['DEVICE_HOST']=\"localhost\"\n",
    "os.environ['DEVICE_ID']=\"58671ff7\" #fill your device-id. Use command \"adb devices\" to get devices names. example :\"e18d5d0\"\n",
    "os.environ['SNPE_TARGET_ARCH']=\"aarch64-android\"\n",
    "os.environ['SNPE_TARGET_STL']=\"libc++_shared.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb36784-a92c-4f4c-ab4d-416d7d25c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL shell \"mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin\" && $DEVICE_SHELL shell \"mkdir -p /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib\" && $DEVICE_SHELL shell \"mkdir -p /data/local/tmp/snpeexample/dsp/lib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ce9e7",
   "metadata": {},
   "source": [
    "* In below code block please use\n",
    "    - v79 for sm8750\n",
    "    - v75 for sm8650\n",
    "    - v73 for sm8550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc06b3-87df-4c97-828f-3a584dc13187",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/$SNPE_TARGET_STL /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib\n",
    "$DEVICE_SHELL push $SNPE_ROOT/bin/$SNPE_TARGET_ARCH/snpe-net-run /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/bin\n",
    "$DEVICE_SHELL push $SNPE_ROOT/lib/hexagon-v79/unsigned/*.so /data/local/tmp/snpeexample/dsp/lib\n",
    "$DEVICE_SHELL push $SNPE_ROOT/lib/$SNPE_TARGET_ARCH/*.so /data/local/tmp/snpeexample/$SNPE_TARGET_ARCH/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88dc68-d1d9-4d3e-8425-f342bbb41d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL shell \"mkdir -p /data/local/tmp/$ONDEVICE_FOLDER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f82c6-456c-4615-9d4a-e6b7d512ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL push $DLCA8W16 /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push $DLCFP32 /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push input_features /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push $TARGET_INPUT_LIST /data/local/tmp/$ONDEVICE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576229af-6f36-4556-9011-4e1115f55354",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL shell\n",
    "chmod -R 777 /data/local/tmp/snpeexample\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/aarch64-android/lib\n",
    "export PATH=$PATH:/data/local/tmp/snpeexample/aarch64-android/bin\n",
    "export OUTPUT_FOLDER=OUTPUT_32b_CPU\n",
    "export OUTPUT_DLC_32=whisper_tiny_encoder_fp32.dlc\n",
    "export ONDEVICE_FOLDER=\"whisper\"\n",
    "cd /data/local/tmp/$ONDEVICE_FOLDER &&\n",
    "snpe-net-run --container $OUTPUT_DLC_32 --input_list list.txt   --output_dir $OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb0d0d-3ce5-4bef-bab5-e66992176d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL shell\n",
    "chmod -R 777 /data/local/tmp/snpeexample\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/data/local/tmp/snpeexample/aarch64-android/lib\n",
    "export PATH=$PATH:/data/local/tmp/snpeexample/aarch64-android/bin\n",
    "export ADSP_LIBRARY_PATH=\"/data/local/tmp/snpeexample/dsp/lib;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp\"\n",
    "export OUTPUT_FOLDER=OUTPUT_DSP_W8A16\n",
    "export DLC_W8A16=whisper_tiny_encoder_w8a16.dlc\n",
    "export ONDEVICE_FOLDER=\"whisper\"\n",
    "cd /data/local/tmp/$ONDEVICE_FOLDER &&\n",
    "snpe-net-run --container $DLC_W8A16 --input_list list.txt  --output_dir $OUTPUT_FOLDER --use_dsp --enable_cpu_fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20e04a-e843-4196-9fed-dd2b0b68a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf OUTPUT_32b_CPU/\n",
    "rm -rf OUTPUT_DSP_W8A16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2a523-8846-4424-b244-aceaf8e5dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL pull /data/local/tmp/$ONDEVICE_FOLDER/OUTPUT_DSP_W8A16 OUTPUT_DSP_W8A16\n",
    "$DEVICE_SHELL pull /data/local/tmp/$ONDEVICE_FOLDER/OUTPUT_32b_CPU OUTPUT_32b_CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f76b3-bef7-4723-86e6-07c3ace34ce3",
   "metadata": {},
   "source": [
    "### Checking the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3381f12-6f20-42c8-aea2-7dddd8086ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "folder = [\"OUTPUT_32b_CPU\",\"OUTPUT_DSP_W8A16\"]\n",
    "\n",
    "\n",
    "for j in range(0,2):\n",
    "    print(\"------------------------------\"+folder[j]+\"------------------------------\")\n",
    "    for result_path in glob.glob(os.path.join(folder[j], '*')):\n",
    "        if \".log\" not in result_path:\n",
    "            last_hidden_state = np.fromfile(result_path+'/599.raw', dtype=\"float32\")\n",
    "            \n",
    "            encoder_hidden_states=last_hidden_state.reshape((1,1500,384))\n",
    "            print(decoder_block_tflite(encoder_hidden_states))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe37b7-39bb-456f-a345-9f011e0bc34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
